{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SundharessB/Natural-Language-Processing/blob/main/2348564_NLPlab1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenization**"
      ],
      "metadata": {
        "id": "1ccf1gKtLRvU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization is the process of converting a large texts into smaller parts called as tokens. This will help us to find new patterns. It is considered as a base step for stemming and lemmatization."
      ],
      "metadata": {
        "id": "iA_eEp6lPD_4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " Write a paragraph based on your interested Domain and that should incorporate the  special characters, punctuations, stop words, negation (don‚Äôt) and emojis (try to add  more than one emoji contiguously in sentence). Perform the following types  of Tokenization and utilize the Python libraries to tokenize."
      ],
      "metadata": {
        "id": "OrwSZVx6LQRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EMOVVWwtQcwC",
        "outputId": "ba6ad22d-d039-4587-c5f4-871742030959"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.tokenize import MWETokenizer\n",
        "from textblob import TextBlob\n",
        "import spacy\n",
        "from gensim.utils import tokenize\n",
        "from keras.preprocessing.text import Tokenizer,text_to_word_sequence"
      ],
      "metadata": {
        "id": "071EfHHHOCdX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "EPXxcv12LCxT"
      },
      "outputs": [],
      "source": [
        "data ='''Sleep is essential for our health and well-being. It helps us to recharge our energy, improve our mood, and strengthen our immune system. However, many people struggle to get enough quality sleep due to various factors, such as stress, noise, or insomnia. üò¥üò¥üò¥\n",
        "\n",
        "One way to improve our sleep is to follow a regular sleep schedule. This means going to bed and waking up at the same time every day, even on weekends. This helps our body to adjust to a natural circadian rhythm, which regulates our sleep-wake cycle. ‚è∞‚è∞‚è∞\n",
        "\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Tokenizer"
      ],
      "metadata": {
        "id": "FacwJhC5b61z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_tokenize(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "euxhrFZKP2i1",
        "outputId": "e4bef268-dfe1-47cc-c8c7-4d29de3cf5e5"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sleep', 'is', 'essential', 'for', 'our', 'health', 'and', 'well-being', '.', 'It', 'helps', 'us', 'to', 'recharge', 'our', 'energy', ',', 'improve', 'our', 'mood', ',', 'and', 'strengthen', 'our', 'immune', 'system', '.', 'However', ',', 'many', 'people', 'struggle', 'to', 'get', 'enough', 'quality', 'sleep', 'due', 'to', 'various', 'factors', ',', 'such', 'as', 'stress', ',', 'noise', ',', 'or', 'insomnia', '.', 'üò¥üò¥üò¥', 'One', 'way', 'to', 'improve', 'our', 'sleep', 'is', 'to', 'follow', 'a', 'regular', 'sleep', 'schedule', '.', 'This', 'means', 'going', 'to', 'bed', 'and', 'waking', 'up', 'at', 'the', 'same', 'time', 'every', 'day', ',', 'even', 'on', 'weekends', '.', 'This', 'helps', 'our', 'body', 'to', 'adjust', 'to', 'a', 'natural', 'circadian', 'rhythm', ',', 'which', 'regulates', 'our', 'sleep-wake', 'cycle', '.', '‚è∞‚è∞‚è∞']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** The outcome of this tokenizer is it's splits the data into words."
      ],
      "metadata": {
        "id": "hiKl_qnqSaxP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sentence Tokenizer"
      ],
      "metadata": {
        "id": "mEtASpFHb9m-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokenize(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_8a11rAQvV0",
        "outputId": "f3110056-7597-47b5-f86e-25ca8e90a4c4"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sleep is essential for our health and well-being.', 'It helps us to recharge our energy, improve our mood, and strengthen our immune system.', 'However, many people struggle to get enough quality sleep due to various factors, such as stress, noise, or insomnia.', 'üò¥üò¥üò¥\\n\\nOne way to improve our sleep is to follow a regular sleep schedule.', 'This means going to bed and waking up at the same time every day, even on weekends.', 'This helps our body to adjust to a natural circadian rhythm, which regulates our sleep-wake cycle.', '‚è∞‚è∞‚è∞']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** This tokenizer splits the data into sentences."
      ],
      "metadata": {
        "id": "CEI3iFxmR9BP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Punctuation Tokenizer"
      ],
      "metadata": {
        "id": "7IRWRj6WcDHM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(wordpunct_tokenize(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hl2VEWCuRt2e",
        "outputId": "cdc8f99a-40e4-4323-ca64-0cd1fd8bf1f0"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sleep', 'is', 'essential', 'for', 'our', 'health', 'and', 'well', '-', 'being', '.', 'It', 'helps', 'us', 'to', 'recharge', 'our', 'energy', ',', 'improve', 'our', 'mood', ',', 'and', 'strengthen', 'our', 'immune', 'system', '.', 'However', ',', 'many', 'people', 'struggle', 'to', 'get', 'enough', 'quality', 'sleep', 'due', 'to', 'various', 'factors', ',', 'such', 'as', 'stress', ',', 'noise', ',', 'or', 'insomnia', '.', 'üò¥üò¥üò¥', 'One', 'way', 'to', 'improve', 'our', 'sleep', 'is', 'to', 'follow', 'a', 'regular', 'sleep', 'schedule', '.', 'This', 'means', 'going', 'to', 'bed', 'and', 'waking', 'up', 'at', 'the', 'same', 'time', 'every', 'day', ',', 'even', 'on', 'weekends', '.', 'This', 'helps', 'our', 'body', 'to', 'adjust', 'to', 'a', 'natural', 'circadian', 'rhythm', ',', 'which', 'regulates', 'our', 'sleep', '-', 'wake', 'cycle', '.', '‚è∞‚è∞‚è∞']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** This tokenizer splits the sentences into words based on punctuations."
      ],
      "metadata": {
        "id": "--JbuMQ4R1LC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "TreebankWord Tokenizer"
      ],
      "metadata": {
        "id": "Ri1IYMWgcSGT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TreebankWordTokenizer()\n",
        "print(tokenizer.tokenize(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEOvP5byTC02",
        "outputId": "3b00e350-f6d3-4156-e2cc-d67128db7390"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sleep', 'is', 'essential', 'for', 'our', 'health', 'and', 'well-being.', 'It', 'helps', 'us', 'to', 'recharge', 'our', 'energy', ',', 'improve', 'our', 'mood', ',', 'and', 'strengthen', 'our', 'immune', 'system.', 'However', ',', 'many', 'people', 'struggle', 'to', 'get', 'enough', 'quality', 'sleep', 'due', 'to', 'various', 'factors', ',', 'such', 'as', 'stress', ',', 'noise', ',', 'or', 'insomnia.', 'üò¥üò¥üò¥', 'One', 'way', 'to', 'improve', 'our', 'sleep', 'is', 'to', 'follow', 'a', 'regular', 'sleep', 'schedule.', 'This', 'means', 'going', 'to', 'bed', 'and', 'waking', 'up', 'at', 'the', 'same', 'time', 'every', 'day', ',', 'even', 'on', 'weekends.', 'This', 'helps', 'our', 'body', 'to', 'adjust', 'to', 'a', 'natural', 'circadian', 'rhythm', ',', 'which', 'regulates', 'our', 'sleep-wake', 'cycle.', '‚è∞‚è∞‚è∞']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** This TreeBankWordTokenizer removes the phrase terminating punctuations like (?,') and retains decimal numbers as single token.\n",
        "\n",
        "For Example in the data given, for the word don't it splitted like \"do\" \"n't\""
      ],
      "metadata": {
        "id": "DEO65_tpTrMj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tweet Tokenizer"
      ],
      "metadata": {
        "id": "9XnC-1UkcWVN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = TweetTokenizer()\n",
        "print(tokenizer.tokenize(data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gZ4LNMw4VlvN",
        "outputId": "4313ad22-fcee-498c-a4c6-a38b6f4608e9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sleep', 'is', 'essential', 'for', 'our', 'health', 'and', 'well-being', '.', 'It', 'helps', 'us', 'to', 'recharge', 'our', 'energy', ',', 'improve', 'our', 'mood', ',', 'and', 'strengthen', 'our', 'immune', 'system', '.', 'However', ',', 'many', 'people', 'struggle', 'to', 'get', 'enough', 'quality', 'sleep', 'due', 'to', 'various', 'factors', ',', 'such', 'as', 'stress', ',', 'noise', ',', 'or', 'insomnia', '.', 'üò¥', 'üò¥', 'üò¥', 'One', 'way', 'to', 'improve', 'our', 'sleep', 'is', 'to', 'follow', 'a', 'regular', 'sleep', 'schedule', '.', 'This', 'means', 'going', 'to', 'bed', 'and', 'waking', 'up', 'at', 'the', 'same', 'time', 'every', 'day', ',', 'even', 'on', 'weekends', '.', 'This', 'helps', 'our', 'body', 'to', 'adjust', 'to', 'a', 'natural', 'circadian', 'rhythm', ',', 'which', 'regulates', 'our', 'sleep-wake', 'cycle', '.', '‚è∞', '‚è∞', '‚è∞']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** It is used to split the emoji's into different words, it can be a base for performing sentimental analysis"
      ],
      "metadata": {
        "id": "1WkcwlzoWo6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Multiword Expression Tokenizer"
      ],
      "metadata": {
        "id": "_q3tfWVgcg3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = MWETokenizer()\n",
        "print(tokenizer.tokenize(word_tokenize(data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rMyf12XIXNeB",
        "outputId": "5b18555c-bdbd-4f60-82ec-7fb8bfb513aa"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sleep', 'is', 'essential', 'for', 'our', 'health', 'and', 'well-being', '.', 'It', 'helps', 'us', 'to', 'recharge', 'our', 'energy', ',', 'improve', 'our', 'mood', ',', 'and', 'strengthen', 'our', 'immune', 'system', '.', 'However', ',', 'many', 'people', 'struggle', 'to', 'get', 'enough', 'quality', 'sleep', 'due', 'to', 'various', 'factors', ',', 'such', 'as', 'stress', ',', 'noise', ',', 'or', 'insomnia', '.', 'üò¥üò¥üò¥', 'One', 'way', 'to', 'improve', 'our', 'sleep', 'is', 'to', 'follow', 'a', 'regular', 'sleep', 'schedule', '.', 'This', 'means', 'going', 'to', 'bed', 'and', 'waking', 'up', 'at', 'the', 'same', 'time', 'every', 'day', ',', 'even', 'on', 'weekends', '.', 'This', 'helps', 'our', 'body', 'to', 'adjust', 'to', 'a', 'natural', 'circadian', 'rhythm', ',', 'which', 'regulates', 'our', 'sleep-wake', 'cycle', '.', '‚è∞‚è∞‚è∞']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = MWETokenizer()\n",
        "tokenizer.add_mwe(('astro','space'))\n",
        "print(tokenizer.tokenize(word_tokenize(data)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mu2PghiyaTqD",
        "outputId": "6ad31502-c076-4787-91d6-d4100361913a"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sleep', 'is', 'essential', 'for', 'our', 'health', 'and', 'well-being', '.', 'It', 'helps', 'us', 'to', 'recharge', 'our', 'energy', ',', 'improve', 'our', 'mood', ',', 'and', 'strengthen', 'our', 'immune', 'system', '.', 'However', ',', 'many', 'people', 'struggle', 'to', 'get', 'enough', 'quality', 'sleep', 'due', 'to', 'various', 'factors', ',', 'such', 'as', 'stress', ',', 'noise', ',', 'or', 'insomnia', '.', 'üò¥üò¥üò¥', 'One', 'way', 'to', 'improve', 'our', 'sleep', 'is', 'to', 'follow', 'a', 'regular', 'sleep', 'schedule', '.', 'This', 'means', 'going', 'to', 'bed', 'and', 'waking', 'up', 'at', 'the', 'same', 'time', 'every', 'day', ',', 'even', 'on', 'weekends', '.', 'This', 'helps', 'our', 'body', 'to', 'adjust', 'to', 'a', 'natural', 'circadian', 'rhythm', ',', 'which', 'regulates', 'our', 'sleep-wake', 'cycle', '.', '‚è∞‚è∞‚è∞']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** Multi Word Expression Tokenizer allows to combine multiple word expressions, it can merge multi word into single tokens"
      ],
      "metadata": {
        "id": "Xe8ExkN-WpAa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Textblob Tokenizer"
      ],
      "metadata": {
        "id": "DuoAfNh1cl3m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "blob = TextBlob(data)\n",
        "words = blob.words\n",
        "print(words)\n",
        "print(len(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yGVb9dBsbGND",
        "outputId": "6211aa28-d728-4025-9702-420ef421d309"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sleep', 'is', 'essential', 'for', 'our', 'health', 'and', 'well-being', 'It', 'helps', 'us', 'to', 'recharge', 'our', 'energy', 'improve', 'our', 'mood', 'and', 'strengthen', 'our', 'immune', 'system', 'However', 'many', 'people', 'struggle', 'to', 'get', 'enough', 'quality', 'sleep', 'due', 'to', 'various', 'factors', 'such', 'as', 'stress', 'noise', 'or', 'insomnia', 'üò¥üò¥üò¥', 'One', 'way', 'to', 'improve', 'our', 'sleep', 'is', 'to', 'follow', 'a', 'regular', 'sleep', 'schedule', 'This', 'means', 'going', 'to', 'bed', 'and', 'waking', 'up', 'at', 'the', 'same', 'time', 'every', 'day', 'even', 'on', 'weekends', 'This', 'helps', 'our', 'body', 'to', 'adjust', 'to', 'a', 'natural', 'circadian', 'rhythm', 'which', 'regulates', 'our', 'sleep-wake', 'cycle', '‚è∞‚è∞‚è∞']\n",
            "90\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** TextBlob is another tokenizer, through the output we can analyse that it will remove the punctuation marks from the data."
      ],
      "metadata": {
        "id": "FHbS2Yz3bkjX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Spacy Tokenizer"
      ],
      "metadata": {
        "id": "P-lIQJczc0s4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "var = spacy.blank('en')\n",
        "doc = var(data)\n",
        "for token in doc:\n",
        "  print(token)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GBySnCktbkTh",
        "outputId": "e32f2a5b-390c-4425-ed6a-f3141534a560"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sleep\n",
            "is\n",
            "essential\n",
            "for\n",
            "our\n",
            "health\n",
            "and\n",
            "well\n",
            "-\n",
            "being\n",
            ".\n",
            "It\n",
            "helps\n",
            "us\n",
            "to\n",
            "recharge\n",
            "our\n",
            "energy\n",
            ",\n",
            "improve\n",
            "our\n",
            "mood\n",
            ",\n",
            "and\n",
            "strengthen\n",
            "our\n",
            "immune\n",
            "system\n",
            ".\n",
            "However\n",
            ",\n",
            "many\n",
            "people\n",
            "struggle\n",
            "to\n",
            "get\n",
            "enough\n",
            "quality\n",
            "sleep\n",
            "due\n",
            "to\n",
            "various\n",
            "factors\n",
            ",\n",
            "such\n",
            "as\n",
            "stress\n",
            ",\n",
            "noise\n",
            ",\n",
            "or\n",
            "insomnia\n",
            ".\n",
            "üò¥\n",
            "üò¥\n",
            "üò¥\n",
            "\n",
            "\n",
            "\n",
            "One\n",
            "way\n",
            "to\n",
            "improve\n",
            "our\n",
            "sleep\n",
            "is\n",
            "to\n",
            "follow\n",
            "a\n",
            "regular\n",
            "sleep\n",
            "schedule\n",
            ".\n",
            "This\n",
            "means\n",
            "going\n",
            "to\n",
            "bed\n",
            "and\n",
            "waking\n",
            "up\n",
            "at\n",
            "the\n",
            "same\n",
            "time\n",
            "every\n",
            "day\n",
            ",\n",
            "even\n",
            "on\n",
            "weekends\n",
            ".\n",
            "This\n",
            "helps\n",
            "our\n",
            "body\n",
            "to\n",
            "adjust\n",
            "to\n",
            "a\n",
            "natural\n",
            "circadian\n",
            "rhythm\n",
            ",\n",
            "which\n",
            "regulates\n",
            "our\n",
            "sleep\n",
            "-\n",
            "wake\n",
            "cycle\n",
            ".\n",
            "‚è∞\n",
            "‚è∞\n",
            "‚è∞\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** Spacy tokenizer provides flexibility to specify special tokens."
      ],
      "metadata": {
        "id": "5k5Ws9J4djQH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gensim Word Tokenizer"
      ],
      "metadata": {
        "id": "Jo9lh2pDc1nV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gensimTokenizer = list(tokenize(data))\n",
        "print(gensimTokenizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5m7TBDTceZLn",
        "outputId": "084b1f6c-06a7-49ac-e806-e6161a8417bc"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Sleep', 'is', 'essential', 'for', 'our', 'health', 'and', 'well', 'being', 'It', 'helps', 'us', 'to', 'recharge', 'our', 'energy', 'improve', 'our', 'mood', 'and', 'strengthen', 'our', 'immune', 'system', 'However', 'many', 'people', 'struggle', 'to', 'get', 'enough', 'quality', 'sleep', 'due', 'to', 'various', 'factors', 'such', 'as', 'stress', 'noise', 'or', 'insomnia', 'One', 'way', 'to', 'improve', 'our', 'sleep', 'is', 'to', 'follow', 'a', 'regular', 'sleep', 'schedule', 'This', 'means', 'going', 'to', 'bed', 'and', 'waking', 'up', 'at', 'the', 'same', 'time', 'every', 'day', 'even', 'on', 'weekends', 'This', 'helps', 'our', 'body', 'to', 'adjust', 'to', 'a', 'natural', 'circadian', 'rhythm', 'which', 'regulates', 'our', 'sleep', 'wake', 'cycle']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization with Keras"
      ],
      "metadata": {
        "id": "hn_9so8lffV2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "notoken = Tokenizer(num_words = 30)\n",
        "notoken.fit_on_texts(data)\n",
        "listOfWords = text_to_word_sequence(data)\n",
        "print(listOfWords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bP2vzYmVfju-",
        "outputId": "cb29f926-9270-403f-ec75-d4c1dd3c1039"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['sleep', 'is', 'essential', 'for', 'our', 'health', 'and', 'well', 'being', 'it', 'helps', 'us', 'to', 'recharge', 'our', 'energy', 'improve', 'our', 'mood', 'and', 'strengthen', 'our', 'immune', 'system', 'however', 'many', 'people', 'struggle', 'to', 'get', 'enough', 'quality', 'sleep', 'due', 'to', 'various', 'factors', 'such', 'as', 'stress', 'noise', 'or', 'insomnia', 'üò¥üò¥üò¥', 'one', 'way', 'to', 'improve', 'our', 'sleep', 'is', 'to', 'follow', 'a', 'regular', 'sleep', 'schedule', 'this', 'means', 'going', 'to', 'bed', 'and', 'waking', 'up', 'at', 'the', 'same', 'time', 'every', 'day', 'even', 'on', 'weekends', 'this', 'helps', 'our', 'body', 'to', 'adjust', 'to', 'a', 'natural', 'circadian', 'rhythm', 'which', 'regulates', 'our', 'sleep', 'wake', 'cycle', '‚è∞‚è∞‚è∞']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Inference:** The advantage of keras tokenizer, is it will convert the data into lower case before tokenizing it."
      ],
      "metadata": {
        "id": "ZPfo8niogxMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusion**\n",
        "\n",
        "The tokens obtained using these tokenizers would vary based on their design and intended use cases. For example, the Tweet Tokenizer is particularly useful for social media text where emoticons and hashtags are prevalent, while the Treebank Word Tokenizer is suited to more structured text following the conventions of the Penn Treebank. Tokenizers from libraries like spaCy and TextBlob not only tokenize but also offer additional linguistic annotations that can be valuable for further processing steps like named entity recognition or part-of-speech tagging, making them suitable for comprehensive natural language understanding tasks. Gensim and Keras tokenizers might be used in text processing before the application of machine learning algorithms, in areas like topic detection or sentiment analysis."
      ],
      "metadata": {
        "id": "iz9TJpcJg_v5"
      }
    }
  ]
}